import torch
from torch import nn




class Head(nn.Module):
    def __init__(self):
        super().__init__()
        
        


class MultiHeadAttention(nn.Module):
    def __init__(self, n_embd, n_head):
        super().__init__()       


    def forward(x):


        return