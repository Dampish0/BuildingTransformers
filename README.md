# Trying to learn math behind attention and learn how to create model architectures  
  
The current model works!
<img width="1274" height="412" alt="image" src="https://github.com/user-attachments/assets/2e738f78-34ae-40b2-b9a2-c34608f8056c" />


My goals and what i have completed:
- ✅ Create a basic transformer model

- ❌ decoder and encoder layers and try to train it to make sure it works.

- ❌ Try to make my own optimizer or just write SGD by manually calculating the gradients, meaning no AutoGrad. Basically doing my own backPropogation

- ❌ try to implement performers according to [https://arxiv.org/abs/2009.14794]

- ❌ Creating my own sub-word tokenizer.
