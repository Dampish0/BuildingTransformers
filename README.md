# Trying to learn math behind attention and learn how to create model architectures  
  
MultiHeadAttention:
<img width="1274" height="412" alt="image" src="https://github.com/user-attachments/assets/2e738f78-34ae-40b2-b9a2-c34608f8056c" />

MLA:
<img width="770" height="302" alt="image" src="https://github.com/user-attachments/assets/28f4c3ae-0ef0-4811-9fb0-2543d46dbc60" />

My goals and what i have completed:
- ✅ Create a basic transformer model

- ✅ Implement deepseek MLA and test it out

- ❌ decoder and encoder layers and try to train it to make sure it works.

- ❌ Try to make my own optimizer or just write SGD by manually calculating the gradients, meaning no AutoGrad. Basically doing my own backPropogation

- ❌ try to implement performers according to [https://arxiv.org/abs/2009.14794]

- ❌ Creating my own sub-word tokenizer.
