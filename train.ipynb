{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "93e00e0c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1115394\n",
      "58\n",
      "[18, 47, 56, 57, 58, 1, 15, 47, 58, 47, 64, 43, 52, 10, 0, 14, 43, 44, 53, 56, 43, 1, 61, 43, 1, 54, 56, 53, 41, 43, 43, 42, 1, 39, 52, 63, 1, 44, 59, 56]\n",
      "['\\n', ' ', '!', '$', '&', \"'\", ',', '-', '.', '3', ':', ';', '?', 'A', 'B', 'C', 'D', 'E', 'F', 'G', 'H', 'I', 'J', 'K', 'L', 'M', 'N', 'O', 'P', 'Q', 'R', 'S', 'T', 'U', 'V', 'W', 'X', 'Y', 'Z', 'a', 'b', 'c', 'd', 'e', 'f', 'g', 'h', 'i', 'j', 'k', 'l', 'm', 'n', 'o', 'p', 'q', 'r', 's', 't', 'u', 'v', 'w', 'x', 'y', 'z']\n",
      "total params: 47.0M params\n",
      "step: 0/3000, loss: 4.349799, t/step: 1.0553s, time left: 3166.02\n",
      "step: 10/3000, loss: 2.917651, t/step: 0.7197s, time left: 2152.03\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[1], line 61\u001b[0m\n\u001b[0;32m     59\u001b[0m optimizer\u001b[38;5;241m.\u001b[39mzero_grad()\n\u001b[0;32m     60\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m _ \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(gradAccum):\n\u001b[1;32m---> 61\u001b[0m     x, y \u001b[38;5;241m=\u001b[39m \u001b[43mget_batch\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     62\u001b[0m     out, loss \u001b[38;5;241m=\u001b[39m model(x, y)\n\u001b[0;32m     63\u001b[0m     loss\u001b[38;5;241m.\u001b[39mbackward()\n",
      "Cell \u001b[1;32mIn[1], line 54\u001b[0m, in \u001b[0;36mget_batch\u001b[1;34m()\u001b[0m\n\u001b[0;32m     52\u001b[0m x \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mstack([torch\u001b[38;5;241m.\u001b[39mtensor(tokens[i:i\u001b[38;5;241m+\u001b[39mblocksize], dtype\u001b[38;5;241m=\u001b[39mtorch\u001b[38;5;241m.\u001b[39mlong) \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m ix])\n\u001b[0;32m     53\u001b[0m y \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mstack([torch\u001b[38;5;241m.\u001b[39mtensor(tokens[i\u001b[38;5;241m+\u001b[39m\u001b[38;5;241m1\u001b[39m:i\u001b[38;5;241m+\u001b[39mblocksize\u001b[38;5;241m+\u001b[39m\u001b[38;5;241m1\u001b[39m], dtype\u001b[38;5;241m=\u001b[39mtorch\u001b[38;5;241m.\u001b[39mlong) \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m ix])\n\u001b[1;32m---> 54\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mx\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mto\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mcuda\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m, y\u001b[38;5;241m.\u001b[39mto(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcuda\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import os\n",
    "import requests\n",
    "import torch\n",
    "import numpy as np\n",
    "import torch.optim.adamw\n",
    "from MLA import Model\n",
    "\n",
    "input_file_path = 'input.txt'\n",
    "if not os.path.exists(input_file_path):\n",
    "    data_url = 'https://raw.githubusercontent.com/karpathy/char-rnn/master/data/tinyshakespeare/input.txt'\n",
    "    with open(input_file_path, 'w', encoding='utf-8') as f:\n",
    "        f.write(requests.get(data_url).text)\n",
    "\n",
    "with open(input_file_path, \"r\", encoding=\"utf-8\") as f:\n",
    "    data = f.read()\n",
    "\n",
    "print(len(data))\n",
    "##data = \"data\"\n",
    "vocab = sorted(list(set(data)))\n",
    "vocab_size = len(vocab)\n",
    "\n",
    "print(vocab.index('t'))\n",
    "\n",
    "tokens = [vocab.index(v) for i, v in enumerate(data)]\n",
    "\n",
    "print(tokens[:40])\n",
    "print(vocab)\n",
    "\n",
    "maxSteps = 3000\n",
    "gradAccum = 1\n",
    "batchSize = 16\n",
    "n_head = 12\n",
    "n_layers = 12\n",
    "n_embd = n_head * 32\n",
    "blocksize = 1024\n",
    "\n",
    "#MHA\n",
    "#model = Model(n_layers,n_embd,n_head,vocab_size,blocksize).to(\"cuda\")\n",
    "\n",
    "model = Model(n_layers,n_embd,n_head,vocab_size,blocksize, LCompression=288, flashATTN=True).to(\"cuda\")\n",
    "#model = torch.compile(model)\n",
    "tot = 0\n",
    "for param in model.parameters():\n",
    "    tot += param.numel()\n",
    "\n",
    "print(f\"total params: {tot//1e6}M params\")\n",
    "\n",
    "\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr=3e-4)\n",
    "def get_batch():\n",
    "    ix = torch.randint(len(tokens) - blocksize, (batchSize,))\n",
    "    x = torch.stack([torch.tensor(tokens[i:i+blocksize], dtype=torch.long) for i in ix])\n",
    "    y = torch.stack([torch.tensor(tokens[i+1:i+blocksize+1], dtype=torch.long) for i in ix])\n",
    "    return x.to(\"cuda\"), y.to(\"cuda\")\n",
    "\n",
    "import time\n",
    "for i in range(maxSteps):\n",
    "    xo = time.time()\n",
    "    optimizer.zero_grad()\n",
    "    for _ in range(gradAccum):\n",
    "        x, y = get_batch()\n",
    "        out, loss = model(x, y)\n",
    "        loss.backward()\n",
    "    optimizer.step()\n",
    "    if i % 10 == 0:\n",
    "        t1 = time.time() - xo\n",
    "        print(f\"step: {i}/{maxSteps}, loss: {loss.item():.6f}, t/step: {t1:.4f}s, time left: {t1 * (maxSteps-i):.2f}\")\n",
    "\n",
    "\n",
    "generated = torch.tensor([0], device=\"cuda\").unsqueeze(0)\n",
    "from torch import nn\n",
    "for i in range(200):\n",
    "    out = model(generated)\n",
    "    out = out[:,-1,:]\n",
    "    \n",
    "    out = nn.functional.softmax(out, dim=-1)\n",
    "    predchar = torch.multinomial(out, num_samples=1)\n",
    "    \n",
    "    \n",
    "    generated = torch.concat([generated, predchar], dim=-1)\n",
    "\n",
    "\n",
    "outText = [vocab[int(v.item())] for i, v in enumerate(generated.squeeze(0))]\n",
    "print(\"\".join(outText))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b7b8841",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".conda",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
