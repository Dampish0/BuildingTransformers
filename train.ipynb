{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "93e00e0c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1115394\n",
      "compiling model....\n",
      "total params: 142.0M params\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "wandb: Currently logged in as: elias-dovkrans (mamichul) to https://api.wandb.ai. Use `wandb login --relogin` to force relogin\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.19.11"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>c:\\Users\\elias\\Documents\\_MAIN_\\Git_Proj\\transformers_learning\\AttentionIsAllYouNeed\\wandb\\run-20250811_155638-da18gs9b</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/mamichul/AttentionIsAllYouNeed/runs/da18gs9b' target=\"_blank\">rural-sky-1</a></strong> to <a href='https://wandb.ai/mamichul/AttentionIsAllYouNeed' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/mamichul/AttentionIsAllYouNeed' target=\"_blank\">https://wandb.ai/mamichul/AttentionIsAllYouNeed</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/mamichul/AttentionIsAllYouNeed/runs/da18gs9b' target=\"_blank\">https://wandb.ai/mamichul/AttentionIsAllYouNeed/runs/da18gs9b</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step: 0/6000, loss: 9.822682, 1754920620.030256, t/step: 18.1936s, time left: 109161.83, LR: 0.000 1e-5, peak mem: 17.96 GB\n",
      "step: 10/6000, loss: 8.579470, 1754920730.8932679, t/step: 11.4071s, time left: 68328.53, LR: 25.000 1e-5, peak mem: 19.10 GB\n",
      "step: 20/6000, loss: 7.669939, 1754920842.7234156, t/step: 11.0061s, time left: 65816.63, LR: 50.000 1e-5, peak mem: 19.10 GB\n",
      "step: 30/6000, loss: 7.449625, 1754920954.7166407, t/step: 10.9835s, time left: 65571.53, LR: 50.000 1e-5, peak mem: 19.10 GB\n",
      "step: 40/6000, loss: 7.563025, 1754921065.6819263, t/step: 10.9845s, time left: 65467.71, LR: 49.999 1e-5, peak mem: 19.10 GB\n",
      "step: 50/6000, loss: 7.490933, 1754921178.4531906, t/step: 11.3167s, time left: 67334.13, LR: 49.997 1e-5, peak mem: 19.10 GB\n",
      "step: 60/6000, loss: 7.528048, 1754921292.960763, t/step: 11.3386s, time left: 67351.41, LR: 49.995 1e-5, peak mem: 19.10 GB\n",
      "step: 70/6000, loss: 7.503152, 1754921405.515489, t/step: 10.8964s, time left: 64615.86, LR: 49.992 1e-5, peak mem: 19.10 GB\n",
      "step: 80/6000, loss: 7.470274, 1754921518.8761842, t/step: 10.9011s, time left: 64534.46, LR: 49.989 1e-5, peak mem: 19.10 GB\n",
      "step: 90/6000, loss: 7.427298, 1754921628.1146126, t/step: 10.6996s, time left: 63234.47, LR: 49.985 1e-5, peak mem: 19.10 GB\n",
      "step: 100/6000, loss: 7.497480, 1754921742.3477986, t/step: 11.3013s, time left: 66677.69, LR: 49.980 1e-5, peak mem: 19.10 GB\n",
      "step: 110/6000, loss: 7.361565, 1754921856.4207025, t/step: 11.3452s, time left: 66823.00, LR: 49.975 1e-5, peak mem: 19.10 GB\n",
      "step: 120/6000, loss: 7.238457, 1754921969.9630098, t/step: 11.2544s, time left: 66175.83, LR: 49.969 1e-5, peak mem: 19.10 GB\n",
      "step: 130/6000, loss: 7.034626, 1754922082.2239642, t/step: 11.0091s, time left: 64623.25, LR: 49.962 1e-5, peak mem: 19.10 GB\n",
      "step: 140/6000, loss: 7.021454, 1754922193.8177967, t/step: 11.0818s, time left: 64939.63, LR: 49.955 1e-5, peak mem: 19.10 GB\n",
      "step: 150/6000, loss: 6.869863, 1754922305.5205548, t/step: 11.1046s, time left: 64961.63, LR: 49.948 1e-5, peak mem: 19.10 GB\n",
      "step: 160/6000, loss: 6.934706, 1754922417.0591202, t/step: 11.0678s, time left: 64636.03, LR: 49.939 1e-5, peak mem: 19.10 GB\n",
      "step: 170/6000, loss: 6.800255, 1754922528.4797633, t/step: 11.0590s, time left: 64474.08, LR: 49.930 1e-5, peak mem: 19.10 GB\n",
      "step: 180/6000, loss: 6.708329, 1754922639.8308132, t/step: 11.0379s, time left: 64240.82, LR: 49.921 1e-5, peak mem: 19.10 GB\n",
      "step: 190/6000, loss: 6.664255, 1754922751.0513825, t/step: 11.0577s, time left: 64245.10, LR: 49.910 1e-5, peak mem: 19.10 GB\n",
      "step: 200/6000, loss: 6.577080, 1754922862.864941, t/step: 11.1045s, time left: 64406.34, LR: 49.899 1e-5, peak mem: 19.10 GB\n",
      "step: 210/6000, loss: 6.454930, 1754922974.917707, t/step: 11.3058s, time left: 65460.47, LR: 49.888 1e-5, peak mem: 19.10 GB\n",
      "step: 220/6000, loss: 6.503909, 1754923087.7987337, t/step: 11.1847s, time left: 64647.73, LR: 49.876 1e-5, peak mem: 19.10 GB\n",
      "step: 230/6000, loss: 6.366116, 1754923200.0417879, t/step: 10.9938s, time left: 63434.17, LR: 49.863 1e-5, peak mem: 19.10 GB\n",
      "step: 240/6000, loss: 6.352627, 1754923310.6202426, t/step: 10.9445s, time left: 63040.22, LR: 49.850 1e-5, peak mem: 19.10 GB\n",
      "step: 250/6000, loss: 6.279572, 1754923420.9902356, t/step: 10.9631s, time left: 63037.56, LR: 49.836 1e-5, peak mem: 19.10 GB\n",
      "step: 260/6000, loss: 6.309250, 1754923531.416634, t/step: 10.9567s, time left: 62891.39, LR: 49.821 1e-5, peak mem: 19.10 GB\n",
      "step: 270/6000, loss: 6.210222, 1754923641.7150056, t/step: 10.9462s, time left: 62721.75, LR: 49.806 1e-5, peak mem: 19.10 GB\n",
      "step: 280/6000, loss: 6.092031, 1754923752.3363423, t/step: 10.9851s, time left: 62834.56, LR: 49.790 1e-5, peak mem: 19.10 GB\n",
      "step: 290/6000, loss: 6.132151, 1754923863.1370544, t/step: 10.9908s, time left: 62757.70, LR: 49.774 1e-5, peak mem: 19.10 GB\n",
      "step: 300/6000, loss: 6.065170, 1754923973.7089477, t/step: 10.9835s, time left: 62605.84, LR: 49.757 1e-5, peak mem: 19.10 GB\n",
      "step: 310/6000, loss: 6.022782, 1754924085.555268, t/step: 11.0984s, time left: 63149.75, LR: 49.739 1e-5, peak mem: 19.10 GB\n",
      "step: 320/6000, loss: 6.025732, 1754924197.592456, t/step: 11.0975s, time left: 63034.04, LR: 49.721 1e-5, peak mem: 19.10 GB\n",
      "step: 330/6000, loss: 5.896303, 1754924309.318531, t/step: 11.1028s, time left: 62952.83, LR: 49.702 1e-5, peak mem: 19.10 GB\n",
      "step: 340/6000, loss: 5.923817, 1754924420.8475628, t/step: 10.9251s, time left: 61835.84, LR: 49.683 1e-5, peak mem: 19.10 GB\n",
      "step: 350/6000, loss: 5.986405, 1754924533.5356076, t/step: 11.0038s, time left: 62171.71, LR: 49.663 1e-5, peak mem: 19.10 GB\n",
      "step: 360/6000, loss: 5.980532, 1754924645.8876426, t/step: 11.0573s, time left: 62362.96, LR: 49.642 1e-5, peak mem: 19.10 GB\n",
      "step: 370/6000, loss: 5.867002, 1754924757.4752262, t/step: 11.0718s, time left: 62334.18, LR: 49.621 1e-5, peak mem: 19.10 GB\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import requests\n",
    "import torch\n",
    "import numpy as np\n",
    "import torch.optim.adamw\n",
    "from GearedMLA import Model\n",
    "import bitsandbytes\n",
    "import json\n",
    "from torch import GradScaler\n",
    "\n",
    "torch.set_float32_matmul_precision('high')\n",
    "\n",
    "input_file_path = 'input.txt'\n",
    "if not os.path.exists(input_file_path):\n",
    "    data_url = 'https://raw.githubusercontent.com/karpathy/char-rnn/master/data/tinyshakespeare/input.txt'\n",
    "    with open(input_file_path, 'w', encoding='utf-8') as f:\n",
    "        f.write(requests.get(data_url).text)\n",
    "\n",
    "with open(input_file_path, \"r\", encoding=\"utf-8\") as f:\n",
    "    data = f.read()\n",
    "\n",
    "print(len(data))\n",
    "\n",
    "# from transformers import AutoTokenizer\n",
    "\n",
    "import os\n",
    "save_dir = os.path.join(os.getcwd(), f\"tokenizers/E11a5_Tokenizer{2*8192-4}\")\n",
    "\n",
    "\n",
    "from transformers import AutoTokenizer\n",
    "tokenizer = AutoTokenizer.from_pretrained(save_dir)\n",
    "# tokenizer = AutoTokenizer.from_pretrained(\"gpt2\")\n",
    "\n",
    "# vocab_size = tokenizer.vocab_size\n",
    "# tokens = tokenizer.encode(data)\n",
    "# from datasets import load_from_disk\n",
    "\n",
    "vocab_size = 8192 * 2\n",
    "# tokens = load_from_disk(\"input_ids_only\")[\"train\"]['input_ids']\n",
    "from datasets import load_from_disk\n",
    "import numpy as np, os, json\n",
    "from tqdm import tqdm\n",
    "\n",
    "# Load the input_ids-only dataset (random-access, on-disk)\n",
    "#ds = load_from_disk(\"input_ids_only\")[\"train\"]\n",
    "\n",
    "pad_id = 16380\n",
    "eos_id = 16382\n",
    "\n",
    "# Choose compact dtype\n",
    "max_token_id = 16383\n",
    "dtype = np.uint16 if max_token_id <= np.iinfo(np.uint16).max else np.uint32\n",
    "bin_path = f\"tokens_{np.dtype(dtype).name}.bin\"\n",
    "meta_path = \"tokens_meta.json\"\n",
    "\n",
    "if not (os.path.exists(bin_path) and os.path.exists(meta_path)):\n",
    "    # Pass 1: count total length (strip pads, add EOS per example)\n",
    "    total_len = 0\n",
    "    for ex in tqdm(ds, desc=\"Counting tokens\"):\n",
    "        ids = ex[\"input_ids\"]\n",
    "        if pad_id is not None:\n",
    "            total_len += sum(1 for t in ids if t != pad_id) + 1\n",
    "        else:\n",
    "            total_len += len(ids) + 1\n",
    "\n",
    "    mm = np.memmap(bin_path, mode=\"w+\", dtype=dtype, shape=(total_len,))\n",
    "    pos = 0\n",
    "\n",
    "    # Pass 2: write tokens\n",
    "    for ex in tqdm(ds, desc=\"Writing memmap\"):\n",
    "        ids = ex[\"input_ids\"]\n",
    "        if pad_id is not None:\n",
    "            ids = [t for t in ids if t != pad_id]\n",
    "        ids.append(eos_id)\n",
    "        L = len(ids)\n",
    "        mm[pos:pos+L] = np.asarray(ids, dtype=dtype)\n",
    "        pos += L\n",
    "    mm.flush()\n",
    "\n",
    "    with open(meta_path, \"w\") as f:\n",
    "        json.dump({\"length\": int(total_len), \"dtype\": str(np.dtype(dtype)), \"eos_id\": int(eos_id), \"pad_id\": int(pad_id) if pad_id is not None else None}, f)\n",
    "\n",
    "# Re-open memmap for reading\n",
    "with open(meta_path, \"r\") as f:\n",
    "    meta = json.load(f)\n",
    "N = int(meta[\"length\"])\n",
    "tokens_1d = np.memmap(bin_path, mode=\"r\", dtype=dtype, shape=(N,))\n",
    "\n",
    "def get_batch():\n",
    "    ix = torch.randint(0, N - blocksize - 1, (batchSize,))\n",
    "    X = torch.empty((batchSize, blocksize), dtype=torch.long)\n",
    "    Y = torch.empty((batchSize, blocksize), dtype=torch.long)\n",
    "    for k, i in enumerate(ix.tolist()):\n",
    "        x_np = np.asarray(tokens_1d[i:i+blocksize], dtype=np.int64)\n",
    "        y_np = np.asarray(tokens_1d[i+1:i+1+blocksize], dtype=np.int64)\n",
    "        X[k] = torch.from_numpy(x_np)\n",
    "        Y[k] = torch.from_numpy(y_np)\n",
    "    return X.to(\"cuda\", non_blocking=True), Y.to(\"cuda\", non_blocking=True)\n",
    "\n",
    "\n",
    "\n",
    "# ##data = \"data\"\n",
    "# vocab = sorted(list(set(data)))\n",
    "# vocab_size = len(vocab)\n",
    "\n",
    "# print(vocab.index('t'))\n",
    "\n",
    "# tokens = [vocab.index(v) for i, v in enumerate(data)]\n",
    "\n",
    "# print(tokens[:40])\n",
    "# #print(vocab)\n",
    "\n",
    "TOKSEEN = 524288//2\n",
    "\n",
    "blocksize = 1024\n",
    "maxSteps = 6000\n",
    "batchSize = 16\n",
    "gradAccum = int((TOKSEEN // blocksize) // batchSize)\n",
    "n_head = 8\n",
    "n_layers = 48 #32\n",
    "n_embd = 512 #512\n",
    "\n",
    "import math\n",
    "\n",
    "def getLR(step, max_steps, base_lr=1e-3, min_lr=1e-5, warmup_steps=1000):\n",
    "    if step < warmup_steps:\n",
    "        return base_lr * step / warmup_steps\n",
    "    progress = (step - warmup_steps) / (max_steps - warmup_steps)\n",
    "    progress = min(max(progress, 0.0), 1.0)\n",
    "    cosine_decay = 0.5 * (1 + math.cos(math.pi * progress))\n",
    "    lr = min_lr + (base_lr - min_lr) * cosine_decay\n",
    "    return lr\n",
    "\n",
    "\n",
    "#MHA\n",
    "#model = Model(n_layers,n_embd,n_head,vocab_size,blocksize).to(\"cuda\")\n",
    "print(\"compiling model....\")\n",
    "model = Model(n_layers,n_embd,n_head,vocab_size,blocksize, LCompression=128, flashATTN=True, attn_dropout=0.3).to(\"cuda\")\n",
    "model = torch.compile(model)\n",
    "torch.cuda.empty_cache()\n",
    "\n",
    "tot = 0\n",
    "for param in model.parameters():\n",
    "    tot += param.numel()\n",
    "\n",
    "print(f\"total params: {tot//1e6}M params\")\n",
    "\n",
    "# optimizer = bitsandbytes.optim.AdamW8bit(\n",
    "#     model.parameters(),\n",
    "#     lr=3e-4,\n",
    "#     min_8bit_size=4096,       # not 512\n",
    "#     percentile_clipping=0,    # avoid extra kernel\n",
    "# )\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr=5e-4, fused=True)\n",
    "import wandb\n",
    "wandb.init()\n",
    "# def get_batch():\n",
    "#     ix = torch.randint(len(tokens) - blocksize, (batchSize,))\n",
    "#     x = torch.stack([torch.tensor(tokens[i:i+blocksize], dtype=torch.long) for i in ix])\n",
    "#     y = torch.stack([torch.tensor(tokens[i+1:i+blocksize+1], dtype=torch.long) for i in ix])\n",
    "#     return x.to(\"cuda\"), y.to(\"cuda\")\n",
    "torch.manual_seed(69696969)\n",
    "torch.cuda.reset_peak_memory_stats()\n",
    "import time\n",
    "for i in range(maxSteps):\n",
    "    xo = time.time()\n",
    "    optimizer.zero_grad(set_to_none=True)\n",
    "    for _ in range(gradAccum):\n",
    "        with torch.amp.autocast(\"cuda\", dtype=torch.bfloat16):\n",
    "            x, y = get_batch()\n",
    "            out, loss = model(x, y)\n",
    "        loss.backward()\n",
    "    optimizer.step()\n",
    "\n",
    "    lr = getLR(i, maxSteps, 5e-4, 5e-5, 20)\n",
    "    for param_group in optimizer.param_groups:\n",
    "        param_group[\"lr\"] = lr\n",
    "    if i % 10 == 0:\n",
    "        Mmem = (torch.cuda.max_memory_allocated() / 1e9)\n",
    "        t1 = time.time() - xo\n",
    "        tleft = t1 * (maxSteps-i)\n",
    "        wandb.log({\n",
    "            \"Loss\": loss.item(),\n",
    "            \"lr\": lr,\n",
    "            \"t/step:\": t1,\n",
    "            \"peak mem\": Mmem,\n",
    "            \"T-left\": tleft\n",
    "        }, step=i)\n",
    "        print(f\"step: {i}/{maxSteps}, loss: {loss.item():.6f}, {time.localtime()}, t/step: {t1:.4f}s, time left: {tleft:.2f}, LR: {lr:.2e}, peak mem: {Mmem:.2f} GB\")\n",
    "    torch.cuda.synchronize()\n",
    "# Save model weights\n",
    "torch.save(model.state_dict(), \"E11a5.pt\")\n",
    "\n",
    "# Save config\n",
    "config = {\n",
    "    \"n_layers\": n_layers,\n",
    "    \"n_embd\": n_embd,\n",
    "    \"n_head\": n_head,\n",
    "    \"vocab_size\": vocab_size,\n",
    "    \"blocksize\": blocksize,\n",
    "    \"LCompression\": 128,  # or your actual value\n",
    "    \"flashATTN\": True,\n",
    "    \"attn_dropout\": 0.3,\n",
    "    \"tokenizer\": \"Custom_gpt2\"\n",
    "}\n",
    "with open(\"model_config.json\", \"w\") as f:\n",
    "    json.dump(config, f, indent=2)\n",
    "\n",
    "\n",
    "generated = torch.tensor([0], device=\"cuda\").unsqueeze(0)\n",
    "from torch import nn\n",
    "t0 = time.time()\n",
    "for i in range(400):\n",
    "    out = model(generated)\n",
    "    out = out[:,-1,:]\n",
    "    \n",
    "    out = nn.functional.softmax(out, dim=-1)\n",
    "    predchar = torch.multinomial(out, num_samples=1)\n",
    "    \n",
    "    \n",
    "    generated = torch.concat([generated, predchar], dim=-1)\n",
    "\n",
    "#generated = model.generate(torch.tensor([0], device=\"cuda\").unsqueeze(0), 200)\n",
    "t1 = time.time() - t0\n",
    "print(f\"time to generate: {t1}, tokens per second: {200/t1}\")\n",
    "#outText = \"\".join([vocab[int(v.item())] for i, v in enumerate(generated.squeeze(0))])\n",
    "\n",
    "outText = tokenizer.decode(generated.squeeze(0))\n",
    "print(outText)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1fa84862",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a7e7780",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "torch.cuda.empty_cache()\n",
    "generated = torch.tensor(tokenizer.encode(\"<|bos|>\"), device=\"cuda\").unsqueeze(0)\n",
    "from torch import nn\n",
    "t0 = time.time()\n",
    "for i in range(400):\n",
    "    out = model(generated)\n",
    "    out = out[:,-1,:]\n",
    "    \n",
    "    out = nn.functional.softmax(out, dim=-1)\n",
    "    predchar = torch.multinomial(out, num_samples=1)\n",
    "    \n",
    "    \n",
    "    generated = torch.concat([generated, predchar], dim=-1)\n",
    "\n",
    "#generated = model.generate(torch.tensor([0], device=\"cuda\").unsqueeze(0), 200)\n",
    "t1 = time.time() - t0\n",
    "print(f\"time to generate: {t1}, tokens per second: {200/t1}\")\n",
    "#outText = \"\".join([vocab[int(v.item())] for i, v in enumerate(generated.squeeze(0))])\n",
    "\n",
    "outText = tokenizer.decode(generated.squeeze(0))\n",
    "print(outText)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c7ad640",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "\n",
    "fw = load_dataset(\"HuggingFaceFW/fineweb-edu\", name=\"sample-10BT\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b7b8841",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from transformers import AutoTokenizer\n",
    "\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"gpt2\")\n",
    "\n",
    "# retrain\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40ac4f95",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ...existing code...\n",
    "from transformers import AutoTokenizer, GPT2TokenizerFast\n",
    "import os\n",
    "\n",
    "# Set your desired vocab size\n",
    "target_vocab_size = ((8192 * 2) - 4)  # change as needed\n",
    "\n",
    "stream = fw[\"train\"]\n",
    "\n",
    "def batch_iterator_hf_sample(dataset_stream, chars_per_batch=10000, text_column=\"text\", max_examples=100000):\n",
    "    buf, n, count = [], 0, 0\n",
    "    for example in dataset_stream:\n",
    "        if count >= max_examples:\n",
    "            break\n",
    "        s = example[text_column].strip()\n",
    "        if not s:\n",
    "            continue\n",
    "        buf.append(s)\n",
    "        n += len(s)\n",
    "        count += 1\n",
    "        if n >= chars_per_batch:\n",
    "            yield \" \".join(buf)\n",
    "            buf, n = [], 0\n",
    "    if buf:\n",
    "        yield \" \".join(buf)\n",
    "\n",
    "# Iterator over your training text (uses `input_file_path` defined earlier)\n",
    "def batch_iterator(path=\"\", chars_per_batch=10000):\n",
    "    with open(path, \"r\", encoding=\"utf-8\") as f:\n",
    "        buf, n = [], 0\n",
    "        for line in f:\n",
    "            s = line.rstrip(\"\\n\")\n",
    "            if not s:\n",
    "                continue\n",
    "            buf.append(s)\n",
    "            n += len(s)\n",
    "            if n >= chars_per_batch:\n",
    "                yield \" \".join(buf)\n",
    "                buf, n = [], 0\n",
    "        if buf:\n",
    "            yield \" \".join(buf)\n",
    "\n",
    "\n",
    "base_tok = GPT2TokenizerFast.from_pretrained(\"gpt2\")\n",
    "new_tok = base_tok.train_new_from_iterator(batch_iterator_hf_sample(stream, max_examples=1000000), vocab_size=target_vocab_size)\n",
    "print(\"training complete\")\n",
    "# # Start from the base GPT-2 tokenizer and train a new one\n",
    "# base_tok = GPT2TokenizerFast.from_pretrained(\"gpt2\")\n",
    "# new_tok = base_tok.train_new_from_iterator(batch_iterator(), vocab_size=target_vocab_size)\n",
    "\n",
    "# Ensure a pad token exists (GPT-2 doesn't have one by default)\n",
    "special_tokens_dict = {\n",
    "    \"pad_token\": \"<|pad|>\",\n",
    "    \"bos_token\": \"<|bos|>\",\n",
    "    \"eos_token\": \"<|eos|>\"\n",
    "}\n",
    "new_tok.add_special_tokens(special_tokens_dict)\n",
    "\n",
    "# Optional: set a practical max length for your use case\n",
    "try:\n",
    "    new_tok.model_max_length = 1024  # uses your existing variable\n",
    "except NameError:\n",
    "    pass\n",
    "\n",
    "# Save tokenizer\n",
    "save_dir = os.path.join(os.getcwd(), f\"tokenizers/E11a5_Tokenizer{target_vocab_size}\")\n",
    "os.makedirs(save_dir, exist_ok=True)\n",
    "new_tok.save_pretrained(save_dir)\n",
    "print(f\"Tokenizer saved to: {save_dir}\")\n",
    "\n",
    "# Load it back and use it\n",
    "tokenizer = AutoTokenizer.from_pretrained(save_dir)\n",
    "print(\"New tokenizer vocab size:\", tokenizer.vocab_size)\n",
    "\n",
    "# Optional: re-encode your data with the new tokenizer\n",
    "# tokens = tokenizer.encode(data)\n",
    "# vocab_size = tokenizer.vocab_size\n",
    "# ...existing code..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cdec2c8d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "save_dir = os.path.join(os.getcwd(), f\"tokenizers/E11a5_Tokenizer{2*8192-4}\")\n",
    "\n",
    "\n",
    "from transformers import AutoTokenizer\n",
    "tokenizer = AutoTokenizer.from_pretrained(save_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b22e40d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tokenize the entire dataset\n",
    "def tokenize_function(example):\n",
    "    import os\n",
    "    target_vocab_size = ((8192 * 2) - 4)  # change as needed\n",
    "\n",
    "    save_dir = os.path.join(os.getcwd(), f\"tokenizers/E11a5_Tokenizer{target_vocab_size}\")\n",
    "\n",
    "\n",
    "    from transformers import AutoTokenizer\n",
    "    tokenizer = AutoTokenizer.from_pretrained(save_dir)\n",
    "    return tokenizer(example[\"text\"], truncation=True, padding=\"max_length\", max_length=1024)\n",
    "\n",
    "# Apply tokenization (batched for speed)\n",
    "tokenized_dataset = fw.map(tokenize_function, batched=True, num_proc=4)\n",
    "\n",
    "# Now tokenized_dataset contains tokenized inputs\n",
    "print(\"done: \",len(tokenized_dataset))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "272b8345",
   "metadata": {},
   "outputs": [],
   "source": [
    "save_path = \"tokenized_fineweb_edu\"\n",
    "tokenized_dataset.save_to_disk(save_path)\n",
    "print(f\"Tokenized dataset saved to: {save_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "658206a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "dd = []\n",
    "for i in range(10):\n",
    "    dd += tokenizer.encode(str(i))\n",
    "    print(i)\n",
    "\n",
    "print(dd)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36f7c789",
   "metadata": {},
   "outputs": [],
   "source": [
    "f = tokenizer.decode(dd)\n",
    "print(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e0cec07c",
   "metadata": {},
   "outputs": [],
   "source": [
    "8192*2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e7a810ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer.special_tokens_map"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "67c0fcab",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Before:\", tokenizer.vocab_size)\n",
    "tokenizer.add_special_tokens({\"pad_token\": \"<|pad|>\"})\n",
    "print(\"After:\", tokenizer.vocab_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d5874d20",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"input_ids.txt\", \"w\", encoding=\"utf-8\") as f:\n",
    "    for example in tokenized_dataset[\"train\"]:\n",
    "        f.write(\" \".join(map(str, example[\"input_ids\"])) + \"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bcf1d560",
   "metadata": {},
   "outputs": [],
   "source": [
    "total_tokens = sum(len(example[\"input_ids\"]) for example in tokenized_dataset[\"train\"])\n",
    "print(\"Total tokens:\", total_tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ab05e2b",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"{(9672101 * 1024 / 1e9):.2f} B\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba2c248e",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer.special_tokens_map\n",
    "tokenizer.encode(\"<|eos|>\")#16382\n",
    "tokenizer.encode(\"<|pad|>\")#16380\n",
    "tokenizer.encode(\"<|bos|>\")#16381\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "274560ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "\n",
    "fw = load_dataset(\"tokenized_fineweb_edu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60e06b1c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ...existing code...\n",
    "# Keep only 'input_ids' column\n",
    "input_ids_dataset = fw.remove_columns(\n",
    "    [col for col in fw[\"train\"].column_names if col != \"input_ids\"]\n",
    ")\n",
    "\n",
    "# Save to disk (Arrow format, fast and efficient)\n",
    "input_ids_dataset.save_to_disk(\"input_ids_only\")\n",
    "# ...existing code..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20a3b33a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm\n",
    "\n",
    "eos_token_id = tokenizer.eos_token_id\n",
    "with open(\"input_ids.txt\", \"w\", encoding=\"utf-8\") as f:\n",
    "    for example in tqdm(fw[\"train\"], desc=\"Writing input_ids\"):\n",
    "        ids = example[\"input_ids\"] + [eos_token_id]\n",
    "        f.write(\" \".join(map(str, ids)) + \"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc7e1dff",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ...existing code...\n",
    "eos_token_id = 16382\n",
    "with open(\"input_ids.txt\", \"w\", encoding=\"utf-8\") as f:\n",
    "    for example in tokenized_dataset[\"train\"]:\n",
    "        ids = example[\"input_ids\"] + [eos_token_id]\n",
    "        f.write(\" \".join(map(str, ids)) + \"\\n\")\n",
    "# ...existing code..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1462f026",
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "for i in range(24):\n",
    "    print(i, 2**i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a6038e98",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".conda",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
